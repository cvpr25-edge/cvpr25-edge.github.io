<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>CVPR25 EDGE Workshop</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM" crossorigin="anonymous">
    <style>
        .responsive-image {
            width: 50%;
            height: auto;
        }
        .responsive-image img {
            width: 100%;
            height: auto;
        }
    </style>
  </head>

  <body>

    <div class="container">
    <header class="d-flex flex-wrap justify-content-center py-3 mb-4 border-bottom">
      

<!--      <img src="edge_pics/cvpr_2024.jpg" width="600">-->

	<div class="responsive-image">
        <img src="edge_pics/cvpr_2025.jpg" alt="CVPR 2025">
    </div>
		<rect width="100%" height="100%"></rect>
        <span> </span> 
		
		<div class="text-center">
		<span class="fs-1"><b>The 2nd Workshop on <span style="color:#48BF91">E</span>fficient and On-<span style="color:#48BF91">D</span>evice <span style="color:#48BF91">Ge</span>neration (<span style="color:#48BF91">EDGE</span>)</b> </span>
		</div>
		 
		<ul class="nav nav-pills">
			<h2>CVPR 2025, Nashville, Tennessee</h2>
	
<!--      <li class="nav-item"><a href="#papers" class="nav-link active" aria-current="page">Papers</a></li>-->
<!--      <li class="nav-item"><a href="#speakers" class="nav-link active" aria-current="page">Speakers</a></li>-->
      </ul>
    </header>
  </div>



<!--<br>-->
<!--<hr>-->
<!--<br>-->
<div class="text-center" id="overview">
<h1>Overview and Call for Papers</h1>
</div>
<br>

<div class="container">
<p class="lead">

The Second Workshop on <strong>E</strong>fficient and On-<strong>D</strong>evice <strong>Ge</strong>neration (<strong>EDGE</strong>) at CVPR 2025 will focus on the latest advancements of generative AI in the computer vision domain, with an emphasis on efficiencies across multiple aspects. We encourage techniques that enable generative models to be trained more efficiently and run on resource-constrained devices, such as mobile phones and edge devices. Through these efforts, we envision a future where these permeating generative AI capabilities become significantly more accessible with virtuous scalability and plateauing carbon footprint. 
</p>

<p class="lead">
The topics involved in the workshop include but are not limited to:
</p>

    <ul class="lead">

    <li> <strong>Training efficiency:</strong> </li>
	
		<ul>
		<li> Methods for reducing the memory footprint of generative models. </li>
		<li> Using knowledge distillation to support model compression. </li>
		<li> Hardware-aware training acceleration and energy efficient computing. </li>
		<li> Online training under edge constraints. </li>
		<li> Parameter efficient tuning. </li>	
		<li> Mixed pre-training to enable downstream tasks via lightweight tuning. </li>
		</ul>

    <li> <strong>Modeling efficiency:</strong> </li>
	
		<ul>
		<li> Design of efficient modules and algorithms for generative models. </li>
		<li> Efficient architecture for generative models: auto-encoders, diffusion, GAN, autoregressive models, etc. </li>
		<li> Efficient transformers and state-space models. </li>
		</ul>

    <li> <strong>Inference efficiency:</strong> </li>
		
		<ul>
		<li> Techniques for reducing the computational complexity of generative models, such as model compression, quantization, sparsification, and distillation. </li>
		<li> Advanced sampling techniques for fast inference. </li>
		<li> Approaches for efficient deployment and distribution generative models across multiple devices or edge devices. </li>	
		<li> Hardware-aware inference acceleration and energy efficient computing. </li>
		</ul>
		

    <li> <strong>Data efficiency:</strong> </li>
					
		<ul>
		<li> Few-shot diffusion model. </li>
		<li> Generative model finetuning. </li>
		<li> Training generative models with fewer data. </li>					
		</ul>					

    <li> <strong>Evaluation efficiency:</strong> </li>
					
		<ul>
		<li> Scalable evaluation methods of generative models. </li>
		<li> Automatic evaluation for image and video generation. </li>					
		</ul>		

    <li> <strong>Application-specific efficiency:</strong> </li>
					
		<ul>
		<li> Applications of efficient generative AI in computer vision, such as image / video generation, image / video editing, style transfer, and super-resolution. </li>
		<li> Efficient personalization and post-generation editing. </li>
		<li> Efficient generative models for real-time video synthesis: talking head, vitual try-on, gaming engine, etc. </li>
		<li> Efficient interplay between media generation with multi-modal conditions, such as text2image, text2video, image2video, text+image2video, audio2video, audio+image2video, etc. </li>						
		</ul>
					
    <li> <strong>Other related topics:</strong> </li>
							
		<ul>
		<li> Challenges and opportunities. </li>
		<li> Exploratory direction. </li>
		<li> Privacy considerations. </li>
		</ul>
 
    </ul>


	<p class="lead">
	<strong>Format</strong>: Submissions must use the <a href="https://cvpr.thecvf.com/Conferences/2025/AuthorGuidelines">CVPR 2025 Author Kit for Latex/Word Zip file</a> and follow the CVPR 2025 author instructions and submission policies. Submissions need to be anonymized. The workshop considers two submission tracks: 
  <ul class="lead">
		<li> <strong>Long Paper</strong>: submissions are limited to <strong>8 pages excluding</strong> references; </li>
		<li> <strong>Extended Abstract</strong>: submissions are limited to <strong>4 pages excluding</strong> references. </li>
		
  </ul>
	
	</p>
	
	  
	<p class="lead">
	Only long papers will be included in the CVPR 2025 proceedings.
	</p>
	
	<p class="lead">
	<strong>Submission Site</strong>: TBD </a>
	</p>
	
	
	<p class="lead">
	
<!--		<strong>Submission Due</strong>: <strong><strike>March 22, 2024 (AOE)</strike> </strong> <strong>  March 26, 2024 (AOE) </strong>-->
	<strong>
    Submission Deadline:  
    <span style="color:red;">TBD</span>
  	</strong>
	</p>

	<p class="lead">
	<strong>Workshop Date</strong>: <strong>TBD</strong>
	</p>
	
	<p class="lead">
	<strong>Workshop Location</strong>: <strong>TBD</strong>
	</p>

</div><!-- container-->





 

<br>

<footer class="py-5 text-center text-body-secondary bg-body-tertiary" id="contact">
  <p>Please contact <a href="mailto:felixu@meta.com">Felix Juefei Xu</a> or <a href="mailto:houtingbo@gmail.com">Tingbo Hou</a> if you have questions. Issues about the website can be posed at <a href="https://github.com/cvpr24-edge/cvpr24-edge.github.io/issues">Github issues</a>. </p>
  <p>The webpage is adapted from <a href="https://iccv23-arow.github.io/">ICCV 2023 AROW Workshop</a> webpage created by <a href="https://cdluminate.github.io/">Mo Zhou</a>. The <a href="https://github.com/iccv23-arow/iccv23-arow.github.io">HTML source code</a> is released under the CC-0 license.</p>
  <p>CVPR 2025 EDGE Workshop</p>
  <p class="mb-0">
    <a href="#">Back to top</a>
  </p>
</footer>

  </body>
</html>

