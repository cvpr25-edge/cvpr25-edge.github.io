<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>CVPR25 EDGE Workshop</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM" crossorigin="anonymous">
    <style>
        .responsive-image {
            width: 50%;
            height: auto;
        }
        .responsive-image img {
            width: 100%;
            height: auto;
        }
    </style>
  </head>

  <body>

    <div class="container">
    <header class="d-flex flex-wrap justify-content-center py-3 mb-4 border-bottom">
      

<!--      <img src="edge_pics/cvpr_2024.jpg" width="600">-->

	<div class="responsive-image">
        <img src="edge_pics/cvpr_2025.jpg" alt="CVPR 2025">
    </div>
		<rect width="100%" height="100%"></rect>
        <span> </span> 
		
		<div class="text-center">
		<span class="fs-1"><b>The 2nd Workshop on <span style="color:#afd7ff">E</span>fficient and On-<span style="color:#afd7ff">D</span>evice <span style="color:#afd7ff">Ge</span>neration (<span style="color:#afd7ff">EDGE</span>)</b> </span>
		</div>
		 
		<ul class="nav nav-pills">
			<h2>CVPR 2025, Nashville, Tennessee</h2>
	
<!--      <li class="nav-item"><a href="#papers" class="nav-link active" aria-current="page">Papers</a></li>-->
<!--      <li class="nav-item"><a href="#speakers" class="nav-link active" aria-current="page">Speakers</a></li>-->
      </ul>
    </header>
  </div>



<!--<br>-->
<!--<hr>-->
<!--<br>-->
<div class="text-center" id="overview">
<h1>Overview and Call for Papers</h1>
</div>
<br>

<div class="container">
<p class="lead">

The Second Workshop on <strong>E</strong>fficient and On-<strong>D</strong>evice <strong>Ge</strong>neration (<strong>EDGE</strong>) at CVPR 2025 will focus on the latest advancements of generative AI in the computer vision domain, with an emphasis on efficiencies across multiple aspects. We encourage techniques that enable generative models to be trained more efficiently and run on resource-constrained devices, such as mobile phones and edge devices. Through these efforts, we envision a future where these permeating generative AI capabilities become significantly more accessible with virtuous scalability and plateauing carbon footprint. 
</p>

<p class="lead">
The topics involved in the workshop include but are not limited to:
</p>

    <ul class="lead">

    <li> <strong>Modeling efficiency:</strong> </li>
	
		<ul>
		<li> Design of efficient modules and algorithms for generative models. </li>
		<li> Efficient architecture for generative models: auto-encoders, diffusion, GAN, autoregressive models, etc. </li>
		<li> Efficient transformers and state-space models. </li>
		</ul>
    
    <li> <strong>Training efficiency:</strong> </li>
	
		<ul>
		<li> Methods for reducing the memory footprint of training generative models. </li>
		<li> Using knowledge distillation to support model compression. </li>
		<li> Hardware-aware training acceleration and energy efficient computing. </li>
		<li> Online training under edge constraints. </li>
		<li> Parameter efficient tuning. </li>	
		<li> Mixed pre-training to enable downstream tasks via lightweight tuning. </li>
		</ul>
	    
    <li> <strong>Inference efficiency:</strong> </li>
		
		<ul>
		<li> Techniques for reducing the computational complexity of generative models, such as quantization, sparsification, and MoEs. </li>
		<li> Advanced sampling techniques for fast inference. </li>
		<li> Approaches for efficient deployment and distribution generative models across multiple devices or edge devices. </li>	
		<li> Hardware-aware inference acceleration and energy efficient computing. </li>
		</ul>
		

    <li> <strong>Data efficiency:</strong> </li>
					
		<ul>
		<li> Few-shot diffusion model. </li>
		<li> Generative model finetuning. </li>
		<li> Training generative models with fewer data. </li>					
		</ul>					

    <li> <strong>Application-specific efficiency:</strong> </li>
					
		<ul>
		<li> Applications of efficient generative AI in computer vision, such as image / video generation, image / video editing, style transfer, and super-resolution. </li>
		<li> Efficient personalization and post-generation editing. </li>
		<li> Efficient generative models for real-time video synthesis: talking head, vitual try-on, gaming engine, etc. </li>						
		</ul>
 
    </ul>


	<p class="lead">
	<strong>Format</strong>: Submissions must use the <a href="https://cvpr.thecvf.com/Conferences/2025/AuthorGuidelines">CVPR 2025 Author Kit for Latex/Word Zip file</a> and follow the CVPR 2025 author instructions and submission policies. Submissions need to be anonymized. The workshop considers two submission tracks: 
  <ul class="lead">
		<li> <strong>Long Paper</strong>: submissions are limited to <strong>8 pages excluding</strong> references; </li>
		<li> <strong>Extended Abstract</strong>: submissions are limited to <strong>4 pages excluding</strong> references. </li>
		
  </ul>
	
	</p>
	
	  
	<p class="lead">
	Only long papers will be included in the CVPR 2025 proceedings.
	</p>
	
	<p class="lead">
	<strong>Submission Site</strong>: TBD </a>
	</p>
	
	
	<p class="lead">
	
<!--		<strong>Submission Due</strong>: <strong><strike>March 22, 2024 (AOE)</strike> </strong> <strong>  March 26, 2024 (AOE) </strong>-->
	<strong>
    Submission Deadline:  
    <span style="color:red;">March 21, 2025 (AOE)</span>
  	</strong>
	</p>

	<p class="lead">
	<strong>Workshop Date</strong>: <strong>TBD</strong>
	</p>
	
	<p class="lead">
	<strong>Workshop Location</strong>: <strong>TBD</strong>
	</p>

</div><!-- container-->









<br>
<hr>
<br>
<div class="text-center" id="speakers">
<h1>Speakers</h1>
</div>
<br>


<div class="container text-center">
    <!-- Three columns of text below the carousel -->
    <div class="row">


      <div class="col">
        <a href="https://cs.stanford.edu/~ermon/">
        <img class="rounded-circle" width="200" height="200" src="edge_pics/ermon.jpg"><rect width="100%" height="100%"></rect></img>
        <h3 class="fw-normal">Stefano Ermon</h3>
        </a>
        <p style="color:grey">Stanford</p>
      </div><!-- /div -->
    
      <div class="col">
				<a href="https://liuziwei7.github.io/">
        <img class="rounded-circle" width="200" height="200" src="edge_pics/ziwei.jpg"><rect width="100%" height="100%"></rect></img>
        <h3 class="fw-normal">Ziwei Liu</h3>
				</a>
        <p style="color:grey">Nanyang Technological University</p>
      </div><!-- /div -->

     <div class="col">
        <a href="https://imisra.github.io/">
        <img class="rounded-circle" width="200" height="200" src="edge_pics/ishan.jpg"><rect width="100%" height="100%"></rect></img>
        <h3 class="fw-normal">Ishan Misra</h3>
        </a>
        <p style="color:grey">GenAI at Meta</p>
      </div><!-- /div -->

	      <div class="col">
		<a href="https://stulyakov.com/">
        <img class="rounded-circle" width="200" height="200" src="edge_pics/sergey.jpg"><rect width="100%" height="100%"></rect></img>
        <h3 class="fw-normal">Sergey Tulyakov</h3>
		</a>
        <p style="color:grey">Snap Inc.</p>
      </div><!-- /div -->

      <div class="col">
        <a href="https://tsong.me/">
        <img class="rounded-circle" width="200" height="200" src="edge_pics/jiaming.jpg"><rect width="100%" height="100%"></rect></img>
        <h3 class="fw-normal">Jiaming Song</h3>
        </a>
        <p style="color:grey">Luma AI</p>
      </div><!-- /div -->

      <div class="col">
				<a href="https://hanlab.mit.edu/songhan">
        <img class="rounded-circle" width="200" height="200" src="edge_pics/song.jpg"><rect width="100%" height="100%"></rect></img>
        <h3 class="fw-normal">Song Han</h3>
				</a>
        <p style="color:grey">Massachusetts Institute of Technology</p>
      </div><!-- /div -->

<!-- 
		<div class="col">
				<a href="https://mingyuliu.net/">
        <img class="rounded-circle" width="200" height="200" src="edge_pics/mingyu.jpg"><rect width="100%" height="100%"></rect></img>
        <h3 class="fw-normal">Ming-Yu Liu</h3>
				</a>
        <p style="color:grey">NVIDIA</p>
      </div>
-->
      <div class="col">
        <a href="https://www.cs.cmu.edu/~junyanz/">
        <img class="rounded-circle" width="auto" height="200" src="edge_pics/junyan.jpg"><rect width="100%" height="100%"></rect></img>
        <h3 class="fw-normal">Jun-Yan Zhu</h3>
        </a>
        <p style="color:grey">Carnegie Mellon University</p>
      </div><!-- /div -->



      <div class="col">
        <a href="https://scholar.google.com/citations?user=HWFvq_wAAAAJ&hl=en">
        <img class="rounded-circle" width="200" height="200" src="edge_pics/jianchao.png"><rect width="100%" height="100%"></rect></img>
        <h3 class="fw-normal">Jianchao Yang</h3>
        </a>
        <p style="color:grey">ByteDance</p>
      </div><!-- /div -->










<!-- 
<br>
<hr>
<br>
<div class="text-center" id="accepted-papers">
<h1>Accepted Papers</h1>
</div>
<br>

<div class="container">
    <p class="lead">
        We are pleased to announce the accepted papers for the First Workshop on Efficient and On-Device Generation (EDGE) at CVPR 2024. Congratulations to all authors!
    </p>

    <h2>Long Papers</h2>
    <ul class="lead">
        <li>
            <strong>Title:</strong> LD-Pruner: Efficient Pruning of Latent Diffusion Models using Task-Agnostic Insights<br>
            <strong>Authors:</strong> Thibault Castells (Nota Inc.), Hyoung-Kyu Song (Nota Inc.), Bo-Kyeong Kim (Nota Inc.), Shinkook Choi (Nota Inc.)<br>
			<strong> <span style="color:red;">[Poster # 36] </span></strong>
        </li>
        <li>
            <strong>Title:</strong> EdgeRelight360: Text-Conditioned 360-Degree HDR Image Generation for Real-Time On-Device Video Portrait Relighting<br>
            <strong>Authors:</strong> Min-Hui Lin (Qualcomm Technologies Inc.), Mahesh Kumar Krishna Reddy (Qualcomm Technologies Inc), Guillaume Berger (Qualcomm Technologies Inc.), Michel Sarkis (Qualcomm Technologies Inc.), Fatih Porikli (Qualcomm AI Research), Ning Bi (Qualcomm) <br>
			<strong> <span style="color:red;">[Poster # 37] </span></strong>

        </li>
    </ul>

    <h2>Short Papers</h2>
    <ul class="lead">
        <li>
            <strong>Title:</strong> FoLD: Efficient Fourier-series-based Score Estimation for Langevin Diffusion<br>
            <strong>Authors:</strong> Siddarth Asokan (Microsoft Research Lab India), Aadithya Srikanth (Indian Institute of Science), Nishanth Shetty (Indian Institute of Science), Chandra Sekhar Seelamantula (IISc Bangalore)<br>
            <strong> <span style="color:red;">[Poster # 38] </span></strong>
        </li>
        <li>
            <strong>Title:</strong> EdgeFusion: On-Device Text-to-Image Generation<br>
            <strong>Authors:</strong> Thibault Castells (Nota Inc.) Hyoung-Kyu Song (Nota Inc.), Tairen Piao (Nota Inc), Shinkook Choi (Nota Inc.), Bo-Kyeong Kim (Nota Inc.), Yim Hanyoung (Samsung Electronics.), Changgwun Lee (Samsung Electronics), Jae Gon Kim (Samsung Electronics), Tae-Ho Kim (Nota, Inc.)<br>
            <strong> <span style="color:red;">[Poster # 39] </span></strong>
        </li>
        <li>
            <strong>Title:</strong> Observation-Guided Diffusion Probabilistic Models<br>
            <strong>Authors:</strong> Junoh Kang (Seoul National University), Jinyoung Choi (Seoul National University), Sungik Choi (LG AI Research), Bohyung Han (Seoul National University)<br>
            <strong> <span style="color:red;">[Poster # 40] </span></strong>
        </li>
        <li>
            <strong>Title:</strong> Efficiently Quantize Latent Diffusion Models<br>
            <strong>Authors:</strong> Yuewei Yang (Meta Platforms Inc.), Jialiang Wang (Meta Platforms Inc), Xiaoliang Dai (Meta Platforms Inc.), Peizhao Zhang (Meta), Hongbo Zhang (Meta)<br>
            <strong> <span style="color:red;">[Poster # 41] </span></strong>
        </li>
        <li>
            <strong>Title:</strong> Knowledge Distillation Cross Domain Diffusion Models for Weakly Supervised Defect Detection<br>
            <strong>Authors:</strong> Yuan-Fu Yang (National Yang Ming Chiao Tung University), Min Sun (NTHU) <br>
            <strong> <span style="color:red;">[Poster # 42] </span></strong>
        </li>
        <li>
            <strong>Title:</strong> Bigger is not Always Better: Scaling Properties of Latent Diffusion Models<br>
            <strong>Authors:</strong> Kangfu Mei (Johns Hopkins University), Zhengzhong Tu (University of Texas at Austin), Mauricio Delbracio (Google Research), Hossein Talebi (Google Inc.), Vishal Patel (Johns Hopkins University), Peyman Milanfar (Google)<br>
            <strong> <span style="color:red;">[Poster # 43] </span></strong>
        </li>
        <li>
            <strong>Title:</strong> ECLIPSE: A Resource-Efficient Text-to-Image Prior for Image Generations<br>
            <strong>Authors:</strong> Maitreya Patel (ASU), Changhoon Kim (Arizona State University), Sheng Cheng (Arizona State University), Chitta Baral (Arizona State University), Yezhou Yang (Arizona State University) <br>
            <strong> <span style="color:red;">[Poster # 44] </span></strong>
        </li>
    </ul>
</div>
-->


<br>
<hr>
<br>
<div class="text-center" id="organizers">
<h1>Organizing Committee</h1>
</div>
<br>


<div class="container text-center">
    <!-- Three columns of text below the carousel -->
    <div class="row">

      <div class="col">
        <a href="https://xujuefei.com/">
        <img class="rounded-circle" width="150" height="auto" src="edge_pics/felix.jpg"><rect width="100%" height="100%"></rect></img>
        <h5 class="fw-normal">Felix Juefei-Xu*</h5>
		</a>
        <p style="color:grey">GenAI, Meta</p>
      </div><!-- /div -->
      <div class="col">
		<a href="https://tbhou.github.io/">
        <img class="rounded-circle" width="150" height="auto" src="edge_pics/tingbo.jpg"><rect width="100%" height="100%"></rect></img>
        <h5 class="fw-normal">Tingbo Hou*</h5>
		</a>
        <p style="color:grey">GenAI, Meta</p>
      </div><!-- /div -->
      <div class="col">

		<a href="https://sites.google.com/view/zhao-yang/">
        <img class="rounded-circle" width="150" height="auto" src="edge_pics/yang.jpg"><rect width="100%" height="100%"></rect></img>
        <h5 class="fw-normal">Yang Zhao</h5>
		</a>
        <p style="color:grey">Google</p>
      </div><!-- /.div -->
      <div class="col">

        <a href="https://lichengunc.github.io/">
        <img class="rounded-circle" width="150" height="auto" src="edge_pics/licheng.jpg"><rect width="100%" height="100%"></rect></img>
        <h5 class="fw-normal">Licheng Yu</h5>
		</a>
        <p style="color:grey">GenAI, Meta</p>
      </div><!-- /div -->
      <div class="col">

		<a href="https://xavierxiao.github.io/">
        <img class="rounded-circle" width="150" height="auto" src="edge_pics/zhisheng.jpg"><rect width="100%" height="100%"></rect></img>
        <h5 class="fw-normal">Zhisheng Xiao</h5>
		</a>
        <p style="color:grey">Google</p>
      </div><!-- /.div -->
      <div class="col">
		

		<a href="https://sites.google.com/view/xiaoliangdai">
        <img class="rounded-circle" width="150" height="auto" src="edge_pics/xiaoliang.jpg"><rect width="100%" height="100%"></rect></img>
        <h5 class="fw-normal">Xiaoliang Dai</h5>
		</a>
        <p style="color:grey">GenAI, Meta</p>
      </div><!-- /div -->
      <div class="col">


		<a href="https://www.linkedin.com/in/qifei-wang-746b9035/">
        <img class="rounded-circle" width="150" height="auto" src="edge_pics/qifei.jpeg"><rect width="100%" height="100%"></rect></img>
        <h5 class="fw-normal">Qifei Wang</h5>
		</a>
        <p style="color:grey">Google DeepMind</p>
      </div><!-- /div -->
      <div class="col">


		<a href="https://sites.google.com/view/taoxu">
        <img class="rounded-circle" width="150" height="auto" src="edge_pics/tao.jpg"><rect width="100%" height="100%"></rect></img>
        <h5 class="fw-normal">Tao Xu</h5>
		</a>
        <p style="color:grey">GenAI, Meta</p>
      </div><!-- /div -->
      <div class="col">

		<a href="https://xuyanwu.github.io/">
        <img class="rounded-circle" width="150" height="auto" src="edge_pics/yanwu.jpg"><rect width="100%" height="100%"></rect></img>
        <h5 class="fw-normal">Yanwu Xu</h5>
		</a>
        <p style="color:grey">Boston University</p>
      </div><!-- /.div -->
      <div class="col">

		<a href="https://www.alithabet.com/">
        <img class="rounded-circle" width="150" height="auto" src="edge_pics/ali.webp"><rect width="100%" height="100%"></rect></img>
        <h5 class="fw-normal">Ali Thabet</h5>
		</a>
        <p style="color:grey">GenAI, Meta</p>
      </div><!-- /.div -->
      <div class="col">

		<a href="https://www.cs.utexas.edu/~lqiang/">
        <img class="rounded-circle" width="150" height="auto" src="edge_pics/qiang.jpg"><rect width="100%" height="100%"></rect></img>
        <h5 class="fw-normal">Qiang Liu</h5>
		</a>
        <p style="color:grey">UT Austin</p>
      </div><!-- /.div -->
      <div class="col">

		<a href="https://juxuan27.github.io/">
        <img class="rounded-circle" width="150" height="auto" src="edge_pics/xuan.jpg"><rect width="100%" height="100%"></rect></img>
        <h5 class="fw-normal">Xuan Ju</h5>
		</a>
        <p style="color:grey">CUHK</p>
      </div><!-- /.div -->
      <div class="col">
		
		<a href="https://ruiqigao.github.io/">
        <img class="rounded-circle" width="150" height="auto" src="edge_pics/ruiqi.jpg"><rect width="100%" height="100%"></rect></img>
        <h5 class="fw-normal">Ruiqi Gao</h5>
		</a>
        <p style="color:grey">Google DeepMind</p>
      </div><!-- /.div -->
      <div class="col">
		

		<a href="https://xiyinmsu.github.io/">
        <img class="rounded-circle" width="150" height="auto" src="edge_pics/xi.jpg"><rect width="100%" height="100%"></rect></img>
        <h5 class="fw-normal">Xi Yin</h5>
		</a>
        <p style="color:grey">GenAI, Meta</p>
      </div><!-- /.div -->
      <div class="col">

		<a href="https://harrypotterrrr.github.io/">
        <img class="rounded-circle" width="150" height="auto" src="edge_pics/haolin.jpeg"><rect width="100%" height="100%"></rect></img>
        <h5 class="fw-normal">Haolin Jia</h5>
		</a>
        <p style="color:grey">Google</p>
      </div><!-- /.div -->
      <div class="col">

		<a href="https://xidexia.github.io/">
        <img class="rounded-circle" width="150" height="auto" src="edge_pics/xide.jpg"><rect width="100%" height="100%"></rect></img>
        <h5 class="fw-normal">Xide Xia</h5>
		</a>
        <p style="color:grey">GenAI, Meta</p>
      </div><!-- /.div -->
      <div class="col">


		<a href="https://scholar.google.com/citations?user=eqQQkM4AAAAJ&hl=en">
        <img class="rounded-circle" width="150" height="auto" src="edge_pics/peizhao.jpg"><rect width="100%" height="100%"></rect></img>
        <h5 class="fw-normal">Peizhao Zhang</h5>
		</a>
        <p style="color:grey">GenAI, Meta</p>
      </div><!-- /.div -->
      <div class="col">
		<a href="https://sites.google.com/site/vajdap">
        <img class="rounded-circle" width="150" height="auto" src="edge_pics/peter.jpg"><rect width="100%" height="100%"></rect></img>
        <h5 class="fw-normal">Peter Vajda</h5>
		</a>
        <p style="color:grey">GenAI, Meta</p>
      </div><!-- /.div -->
 </div><!-- /.row -->
</div><!-- /.container -->

<!--

<br>
<hr>
<br>
<div class="text-center" id="program-committee">
<h1>Program Committee</h1>
</div>
<br>

<div class='container lead'>
<ul>
<li>Alexander Robey (University of Pennsylvania)</li>
<li>Chengzhi Mao (Columbia University)</li>
<li>Dingcheng Yang (Tsinghua University)</li>
<li>Emily Diana (University of Pennsylvania)</li>
<li>Gaurang Sriramanan (University of Maryland, College Park)</li>
<li>Hanxun Huang (The University of Melbourne)</li>
<li>Haoxuanye Ji (Xi'an JiaoTong University)</li>
<li>Jiachen Sun (University of Michigan)</li>
<li>Julia Grabinski (University Siegen)</li>
<li>Junyang Wu (Shanghai Jiao Tong university)</li>
<li>Kibok Lee (Yonsei University)</li>
<li>Muzammal Naseer (MBZUAI)</li>
<li>Pengliang Ji (Beihang University)</li>
<li>Peter Lorenz (Fraunhofer)</li>
<li>Qihao Liu (Johns Hopkins University)</li>
<li>Sahil Verma (University of Washington)</li>
<li>Salah GHAMIZI (University of Luxembourg)</li>
<li>ShengYun Peng (Georgia Institute of Technology)</li>
<li>Simin Li (Beihang University)</li>
<li>Won Park (Aura)</li>
<li>Wufei Ma (Johns Hopkins University)</li>
<li>Xianhang Li (University of California, Santa Cruz)</li>
<li>Xinyu Zhang (Zhejiang University)</li>
<li>Yulong Cao (University of Michigan, Ann Arbor )</li>
<li>Zihao Xiao (Johns Hopkins University)</li>
<li>Ziqi Zhang (Google)</li>
</ul>
</div>

-->


<br>
<hr>
<br>
<div class="text-center" id="authorguide">
<h1>Important Dates</h1>
</div>
<br>

<div class="container lead">

<ul class="lead">

<!--    <li> <strong>Submission Deadline: <strike>March 22, 2024 (AOE)</strike> March 26, 2024 (AOE) </strong></li>-->
<li> 
  <strong>
    Submission Deadline:  
    <span style="color:red;">March 21, 2025 (AOE)</span>
  </strong>
</li>
    <li> <strong>Author Notification: TBD</strong> </li>
				
    <li> <strong>Camera-Ready Deadline: TBD</strong></li>

	<li> <strong>Workshop Date: TBD</strong></li>
</ul>
</div>



<!--

<div class="container text-center lead">
<h1>Information for Presenters</h1>
</div>

<div class="container lead">
<ol>
    <li>
    <p>Invited talk</p>
    <p>Each workshop room will be equipped with A/V systems for presentation, including a HDMI cable to connect one machine (speaker's PC or Mac), 2 microphones on the podium, 2 wireless microphones, and video projectors. The aspect ratio of the screen will be 16:9. 
Please encourage presenters to attend the workshop in person and communicate with participants at the venue. If the presenters are not able to attend in person, remote presentation is also allowed. In this case, no registration is needed for invited speakers. Note, however, that remote presenters with IEEE/CVF workshop papers must have (at least one-day) workshops/tutorials pass registration. Please check out the authors note on the registration page: https://iccv2023.thecvf.com/registration-81.php</p>
    </li>

    <li>
    <p>Poster presentation (95.4 cm x 138.8 cm, portrait format)</p>
    <p>Workshop poster sessions will be held in each workshop room, and poster panels will be provided and arranged inside the room. Note that the poster size for workshops is different from that for the main conference. The workshop poster panel size will be 95.4 cm x 138.8 cm (WxH, aspect ratio 0.69:1, portrait format). A0 paper in portrait would well fit the panel by some margin. Please make sure to notify this information to the workshop poster presenters. 
There will be an on-site printing service from which you can collect your printed poster. You will receive more information about the on-site printing service in a separate email later.</p>
    </li>

    <li>
    <p>Workshop streaming</p>
    <p>If you plan to live-stream your workshop, a Wi-Fi network enough for streaming will be provided for each workshop room. However, we do not provide technical support for workshop streaming, and thus streaming of workshops should be managed by each organizer. </p>
    </li>

    <li>
    <p>Workshop Rooms</p>
    <p> The workshop rooms are already announced on the <a href="https://iccv2023.thecvf.com/list.of.accepted.workshops-90.php">ICCV webpage.</a></p>

    <p>
We have updated some initial room assignments based on the registration survey, so please double-check your room code, although you have done it before.
    </p>
    <p>
Each room of East or West, whose code starts with E or W, accommodates 160~190 seats. It will be equipped with A/V systems, including an HDMI cable to connect one machine (speaker's PC or Mac), 2 microphones on the podium, 2 wireless microphones, and video projectors (8K lumens). The screen's aspect ratio will be 16:9. One technician is assigned to two rooms to help you set up A/V systems.
    </p>

    <p>
Each room of South, whose code starts with S, has 570 seats. It will also be equipped with A/V systems, including an HDMI cable to connect one machine (speaker's PC or Mac), 2 microphones on the podium, 4 wireless microphones, and video projectors (12K lumens). The screen's aspect ratio will be 16:9. One technician is assigned to help you set up A/V systems.
    </p>

    <p>
Some workshops are assigned to the plenary room P01, the largest room, which will be provided with better equipment than the South rooms.
Note that the number of seats in each room may decrease to get space for poster panels.
    </p>

</ol>
</div>
-->






<br>
<hr>
<br>
<div class="text-center" id="sponsor">
<h1>Sponsor</h1>
</div>
<br>

<div class="container text-center">
<h4>Please <a href="#contact">contact us</a> (see bottom of this page) if you are interested in sponsoring this workshop!</h4>

</div><!--container-->


																		   
<!--
																		   
																		   
<br>
<hr>
<br>
<div class="text-center" id="tweets">
<h1>Social Network Updates</h1>
</div>
<br>

<div class='container text-center'>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">📢 [Deadline Extension] Good news! We have extended the submission deadline for the 4th Workshop on Adversarial Robustness In the Real World, ICCV2023!<br><br>📅 New DDL: July 20, 2023, 23:59 PT<br><br>📷 Workshop Website: <a href="https://t.co/r9tu5UvuG8">https://t.co/r9tu5UvuG8</a><a href="https://twitter.com/hashtag/AROW?src=hash&amp;ref_src=twsrc%5Etfw">#AROW</a> <a href="https://twitter.com/hashtag/ICCV2023?src=hash&amp;ref_src=twsrc%5Etfw">#ICCV2023</a> <a href="https://twitter.com/hashtag/AdversarialRobustness?src=hash&amp;ref_src=twsrc%5Etfw">#AdversarialRobustness</a></p>&mdash; M. Zhou (@MZhou73277685) <a href="https://twitter.com/MZhou73277685/status/1681201839092695040?ref_src=twsrc%5Etfw">July 18, 2023</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">📢 [Call For Papers] We invite participants to submit their work to the 4th Workshop on Adversarial Robustness In the Real World, ICCV 2023, France!<br><br>📷 Workshop Website: <a href="https://t.co/r9tu5UvuG8">https://t.co/r9tu5UvuG8</a><a href="https://twitter.com/hashtag/AROW?src=hash&amp;ref_src=twsrc%5Etfw">#AROW</a> <a href="https://twitter.com/hashtag/ICCV2023?src=hash&amp;ref_src=twsrc%5Etfw">#ICCV2023</a> <a href="https://twitter.com/hashtag/AdversarialRobustness?src=hash&amp;ref_src=twsrc%5Etfw">#AdversarialRobustness</a> <a href="https://twitter.com/hashtag/DeepLearning?src=hash&amp;ref_src=twsrc%5Etfw">#DeepLearning</a> <a href="https://twitter.com/hashtag/ComputerVision?src=hash&amp;ref_src=twsrc%5Etfw">#ComputerVision</a> <a href="https://twitter.com/hashtag/Paris?src=hash&amp;ref_src=twsrc%5Etfw">#Paris</a></p>&mdash; M. Zhou (@MZhou73277685) <a href="https://twitter.com/MZhou73277685/status/1675380995456126976?ref_src=twsrc%5Etfw">July 2, 2023</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">📢 Exciting news! Join us at the 4th Workshop on Adversarial Robustness In the Real World, happening at ICCV 2023 in Paris, France. 🌍🤖<br><br>🔗 Workshop Website: <a href="https://t.co/ELnVUJyg3H">https://t.co/ELnVUJyg3H</a><a href="https://twitter.com/hashtag/AROW?src=hash&amp;ref_src=twsrc%5Etfw">#AROW</a> <a href="https://twitter.com/hashtag/ICCV2023?src=hash&amp;ref_src=twsrc%5Etfw">#ICCV2023</a> <a href="https://twitter.com/hashtag/AdversarialRobustness?src=hash&amp;ref_src=twsrc%5Etfw">#AdversarialRobustness</a> <a href="https://twitter.com/hashtag/DeepLearning?src=hash&amp;ref_src=twsrc%5Etfw">#DeepLearning</a> <a href="https://twitter.com/hashtag/ComputerVision?src=hash&amp;ref_src=twsrc%5Etfw">#ComputerVision</a> <a href="https://twitter.com/hashtag/Paris?src=hash&amp;ref_src=twsrc%5Etfw">#Paris</a></p>&mdash; M. Zhou (@MZhou73277685) <a href="https://twitter.com/MZhou73277685/status/1675377802349264896?ref_src=twsrc%5Etfw">July 2, 2023</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>
-->






 

<br>

<footer class="py-5 text-center text-body-secondary bg-body-tertiary" id="contact">
  <p>Please contact <a href="mailto:juefei.xu@gmail.com">Felix Juefei Xu</a> or <a href="mailto:houtingbo@gmail.com">Tingbo Hou</a> if you have questions. Issues about the website can be posed at <a href="https://github.com/cvpr24-edge/cvpr24-edge.github.io/issues">Github issues</a>. </p>
  <p>The webpage is adapted from <a href="https://iccv23-arow.github.io/">ICCV 2023 AROW Workshop</a> webpage created by <a href="https://cdluminate.github.io/">Mo Zhou</a>. The <a href="https://github.com/iccv23-arow/iccv23-arow.github.io">HTML source code</a> is released under the CC-0 license.</p>
  <p>CVPR 2025 EDGE Workshop</p>
  <p class="mb-0">
    <a href="#">Back to top</a>
  </p>
</footer>

  </body>
</html>
